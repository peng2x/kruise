PS C:\Git\kruise> kubectl logs kruise-daemon-linux-gs5tt -n kruise-system
I0115 22:27:24.126836       1 feature_gate.go:249] feature gates: &{map[ImagePullJobGate:true]}
I0115 22:27:24.323265       1 daemon.go:101] "Starting daemon" nodeName="aks-agentpool-87866486-vmss000001"
I0115 22:27:24.323629       1 cri.go:44] "Connecting to image service" endpoint="/hostvarrun/containerd/containerd.sock"
I0115 22:27:24.323663       1 util_unix.go:103] "Using this endpoint is deprecated, please consider using full URL format" endpoint="/hostvarrun/containerd/containerd.sock" URL="unix:///hostvarrun/containerd/containerd.sock"
I0115 22:27:24.323746       1 clientconn.go:305] "[core] [Channel #1]Channel created\n"
I0115 22:27:24.323767       1 logging.go:39] "[core] [Channel #1]original dial target is: \"/hostvarrun/containerd/containerd.sock\"\n"
I0115 22:27:24.323822       1 logging.go:39] "[core] [Channel #1]parsed dial target is: resolver.Target{URL:url.URL{Scheme:\"\", Opaque:\"\", User:(*url.Userinfo)(nil), Host:\"\", Path:\"/hostvarrun/containerd/containerd.sock\", RawPath:\"\", OmitHost:false, ForceQuery:false, RawQuery:\"\", Fragment:\"\", RawFragment:\"\"}}\n"
I0115 22:27:24.323836       1 logging.go:39] "[core] [Channel #1]fallback to scheme \"passthrough\"\n"
I0115 22:27:24.323855       1 logging.go:39] "[core] [Channel #1]parsed dial target is: passthrough:////hostvarrun/containerd/containerd.sock\n"
I0115 22:27:24.323869       1 logging.go:39] "[core] [Channel #1]Channel authority set to \"%2Fhostvarrun%2Fcontainerd%2Fcontainerd.sock\"\n"
I0115 22:27:24.324062       1 logging.go:39] "[core] [Channel #1]Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/hostvarrun/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Metadata\": null\n    }\n  ],\n  \"Endpoints\": [\n    {\n      \"Addresses\": [\n        {\n          \"Addr\": \"/hostvarrun/containerd/containerd.sock\",\n          \"ServerName\": \"\",\n          \"Attributes\": null,\n          \"BalancerAttributes\": null,\n          \"Metadata\": null\n        }\n      ],\n      \"Attributes\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
I0115 22:27:24.324098       1 logging.go:39] "[core] [Channel #1]Channel switches to new LB policy \"pick_first\"\n"
I0115 22:27:24.324180       1 pickfirst.go:123] "[core] [pick-first-lb 0xc00076acc0] Received new config {\n  \"shuffleAddressList\": false\n}, resolver state {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/hostvarrun/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Metadata\": null\n    }\n  ],\n  \"Endpoints\": [\n    {\n      \"Addresses\": [\n        {\n          \"Addr\": \"/hostvarrun/containerd/containerd.sock\",\n          \"ServerName\": \"\",\n          \"Attributes\": null,\n          \"BalancerAttributes\": null,\n          \"Metadata\": null\n        }\n      ],\n      \"Attributes\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n}\n"
I0115 22:27:24.324222       1 clientconn.go:846] "[core] [<nil> SubChannel #2]Subchannel created\n"
I0115 22:27:24.324243       1 logging.go:39] "[core] [Channel #1]Channel Connectivity change to CONNECTING\n"
I0115 22:27:24.324266       1 clientconn.go:305] "[core] [Channel #1]Channel exiting idle mode\n"
I0115 22:27:24.324287       1 helpers.go:227] "Finding the CRI API image version"
I0115 22:27:24.324369       1 logging.go:39] "[core] [<nil> SubChannel #2]Subchannel Connectivity change to CONNECTING\n"
I0115 22:27:24.324401       1 logging.go:39] "[core] [<nil> SubChannel #2]Subchannel picks a new address \"/hostvarrun/containerd/containerd.sock\" to connect\n"
I0115 22:27:24.324475       1 pickfirst.go:166] "[core] [pick-first-lb 0xc00076acc0] Received SubConn state update: 0xc00076ad50, {ConnectivityState:CONNECTING ConnectionError:<nil>}\n"
I0115 22:27:24.324619       1 logging.go:39] "[core] [<nil> SubChannel #2]Subchannel Connectivity change to READY\n"
I0115 22:27:24.324635       1 pickfirst.go:166] "[core] [pick-first-lb 0xc00076acc0] Received SubConn state update: 0xc00076ad50, {ConnectivityState:READY ConnectionError:<nil>}\n"
I0115 22:27:24.324645       1 logging.go:39] "[core] [Channel #1]Channel Connectivity change to READY\n"
I0115 22:27:24.325638       1 helpers.go:233] "Using CRI v1 image API"
I0115 22:27:24.326241       1 remote_runtime.go:80] "Connecting to runtime service" endpoint="unix:///hostvarrun/containerd/containerd.sock"
I0115 22:27:24.326294       1 clientconn.go:305] "[core] [Channel #4]Channel created\n"
I0115 22:27:24.326309       1 logging.go:39] "[core] [Channel #4]original dial target is: \"/hostvarrun/containerd/containerd.sock\"\n"
I0115 22:27:24.326334       1 logging.go:39] "[core] [Channel #4]parsed dial target is: resolver.Target{URL:url.URL{Scheme:\"\", Opaque:\"\", User:(*url.Userinfo)(nil), Host:\"\", Path:\"/hostvarrun/containerd/containerd.sock\", RawPath:\"\", OmitHost:false, ForceQuery:false, RawQuery:\"\", Fragment:\"\", RawFragment:\"\"}}\n"
I0115 22:27:24.326350       1 logging.go:39] "[core] [Channel #4]fallback to scheme \"passthrough\"\n"
I0115 22:27:24.326367       1 logging.go:39] "[core] [Channel #4]parsed dial target is: passthrough:////hostvarrun/containerd/containerd.sock\n"
I0115 22:27:24.326380       1 logging.go:39] "[core] [Channel #4]Channel authority set to \"%2Fhostvarrun%2Fcontainerd%2Fcontainerd.sock\"\n"
I0115 22:27:24.326459       1 logging.go:39] "[core] [Channel #4]Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/hostvarrun/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Metadata\": null\n    }\n  ],\n  \"Endpoints\": [\n    {\n      \"Addresses\": [\n        {\n          \"Addr\": \"/hostvarrun/containerd/containerd.sock\",\n          \"ServerName\": \"\",\n          \"Attributes\": null,\n          \"BalancerAttributes\": null,\n          \"Metadata\": null\n        }\n      ],\n      \"Attributes\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
I0115 22:27:24.326479       1 logging.go:39] "[core] [Channel #4]Channel switches to new LB policy \"pick_first\"\n"
I0115 22:27:24.326528       1 pickfirst.go:123] "[core] [pick-first-lb 0xc000577c80] Received new config {\n  \"shuffleAddressList\": false\n}, resolver state {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/hostvarrun/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Metadata\": null\n    }\n  ],\n  \"Endpoints\": [\n    {\n      \"Addresses\": [\n        {\n          \"Addr\": \"/hostvarrun/containerd/containerd.sock\",\n          \"ServerName\": \"\",\n          \"Attributes\": null,\n          \"BalancerAttributes\": null,\n          \"Metadata\": null\n        }\n      ],\n      \"Attributes\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n}\n"
I0115 22:27:24.326555       1 clientconn.go:846] "[core] [<nil> SubChannel #5]Subchannel created\n"
I0115 22:27:24.326571       1 logging.go:39] "[core] [Channel #4]Channel Connectivity change to CONNECTING\n"
I0115 22:27:24.326586       1 clientconn.go:305] "[core] [Channel #4]Channel exiting idle mode\n"
I0115 22:27:24.326598       1 remote_runtime.go:136] "Validating the CRI v1 API runtime version"
I0115 22:27:24.326650       1 logging.go:39] "[core] [<nil> SubChannel #5]Subchannel Connectivity change to CONNECTING\n"
I0115 22:27:24.326665       1 logging.go:39] "[core] [<nil> SubChannel #5]Subchannel picks a new address \"/hostvarrun/containerd/containerd.sock\" to connect\n"
I0115 22:27:24.326752       1 pickfirst.go:166] "[core] [pick-first-lb 0xc000577c80] Received SubConn state update: 0xc000577f50, {ConnectivityState:CONNECTING ConnectionError:<nil>}\n"
I0115 22:27:24.326796       1 logging.go:39] "[core] [<nil> SubChannel #5]Subchannel Connectivity change to READY\n"
I0115 22:27:24.326809       1 pickfirst.go:166] "[core] [pick-first-lb 0xc000577c80] Received SubConn state update: 0xc000577f50, {ConnectivityState:READY ConnectionError:<nil>}\n"
I0115 22:27:24.326816       1 logging.go:39] "[core] [Channel #4]Channel Connectivity change to READY\n"
I0115 22:27:24.423113       1 remote_runtime.go:143] "Validated CRI v1 runtime API"
I0115 22:27:24.423457       1 factory.go:130] "Add runtime" runtimeName="containerd" runtimeURI="" runtimeRemoteURI="unix:///hostvarrun/containerd/containerd.sock"
I0115 22:27:24.424495       1 main.go:85] "No plugin config file found, skipping" configFile="/kruise/CredentialProviderPlugin.yaml"
I0115 22:27:24.424512       1 parse.go:31] make and set new docker keyring
I0115 22:27:24.424523       1 plugins.go:73] Registering credential provider: .dockercfg
I0115 22:27:24.424614       1 reflector.go:289] Starting reflector *v1.Pod (0s) from pkg/mod/k8s.io/client-go@v0.28.9/tools/cache/reflector.go:229
I0115 22:27:24.424624       1 reflector.go:325] Listing and watching *v1.Pod from pkg/mod/k8s.io/client-go@v0.28.9/tools/cache/reflector.go:229
I0115 22:27:24.624880       1 shared_informer.go:341] caches populated
I0115 22:27:24.624928       1 container_meta_controller.go:202] Starting containermeta Controller
I0115 22:27:24.624946       1 container_meta_controller.go:211] Started containermeta Controller successfully
I0115 22:27:24.624988       1 pod_probe_controller.go:196] Starting informer for NodePodProbe
I0115 22:27:24.625030       1 imagepuller_controller.go:153] Starting informer for NodeImage
I0115 22:27:24.625050       1 crr_daemon_controller.go:168] Starting informer for ContainerRecreateRequest
I0115 22:27:24.625088       1 reflector.go:289] Starting reflector *v1alpha1.NodePodProbe (0s) from pkg/mod/k8s.io/client-go@v0.28.9/tools/cache/reflector.go:229
I0115 22:27:24.625097       1 reflector.go:325] Listing and watching *v1alpha1.NodePodProbe from pkg/mod/k8s.io/client-go@v0.28.9/tools/cache/reflector.go:229
I0115 22:27:24.625097       1 reflector.go:289] Starting reflector *v1alpha1.NodeImage (0s) from pkg/mod/k8s.io/client-go@v0.28.9/tools/cache/reflector.go:229
I0115 22:27:24.625099       1 reflector.go:289] Starting reflector *v1alpha1.ContainerRecreateRequest (0s) from pkg/mod/k8s.io/client-go@v0.28.9/tools/cache/reflector.go:229
I0115 22:27:24.625104       1 reflector.go:325] Listing and watching *v1alpha1.NodeImage from pkg/mod/k8s.io/client-go@v0.28.9/tools/cache/reflector.go:229
I0115 22:27:24.625107       1 reflector.go:325] Listing and watching *v1alpha1.ContainerRecreateRequest from pkg/mod/k8s.io/client-go@v0.28.9/tools/cache/reflector.go:229
I0115 22:27:24.725769       1 shared_informer.go:341] caches populated
I0115 22:27:24.725789       1 crr_daemon_controller.go:174] Starting crr daemon controller
I0115 22:27:24.725769       1 shared_informer.go:341] caches populated
I0115 22:27:24.725808       1 shared_informer.go:341] caches populated
I0115 22:27:24.725826       1 imagepuller_controller.go:159] Starting puller controller
I0115 22:27:24.725836       1 imagepuller_controller.go:166] Started puller controller successfully
I0115 22:27:24.725880       1 imagepuller_controller.go:209] "Start syncing" name="aks-agentpool-87866486-vmss000001"
I0115 22:27:24.725813       1 pod_probe_controller.go:202] Starting NodePodProbe controller
I0115 22:27:24.726016       1 pod_probe_controller.go:214] Started NodePodProbe controller successfully
I0115 22:27:24.725828       1 crr_daemon_controller.go:182] Started crr daemon controller successfully
I0115 22:27:24.726321       1 imagepuller_worker.go:80] "sync puller" spec="{\"kind\":\"NodeImage\",\"apiVersion\":\"apps.kruise.io/v1alpha1\",\"metadata\":{\"name\":\"aks-agentpool-87866486-vmss000001\",\"uid\":\"ec375ff0-9cbe-4159-a91a-30d8255c42c2\",\"resourceVersion\":\"18266307\",\"generation\":1,\"creationTimestamp\":\"2025-01-06T22:25:55Z\",\"labels\":{\"agentpool\":\"agentpool\",\"beta.kubernetes.io/arch\":\"amd64\",\"beta.kubernetes.io/instance-type\":\"Standard_D4ds_v5\",\"beta.kubernetes.io/os\":\"linux\",\"failure-domain.beta.kubernetes.io/region\":\"centralus\",\"failure-domain.beta.kubernetes.io/zone\":\"0\",\"kubernetes.azure.com/agentpool\":\"agentpool\",\"kubernetes.azure.com/azure-cni-overlay\":\"true\",\"kubernetes.azure.com/cluster\":\"MC_pepeng-aks-1_pepeng-aks-1_centralus\",\"kubernetes.azure.com/consolidated-additional-properties\":\"8a1777be-cc7c-11ef-9dff-6ef1ad341b01\",\"kubernetes.azure.com/kubelet-identity-client-id\":\"ceff8493-3adf-48dd-b8f6-a771cc585f8d\",\"kubernetes.azure.com/mode\":\"system\",\"kubernetes.azure.com/network-name\":\"aks-vnet-32584312\",\"kubernetes.azure.com/network-policy\":\"none\",\"kubernetes.azure.com/network-resourcegroup\":\"pepeng-aks-1\",\"kubernetes.azure.com/network-subnet\":\"aks-subnet\",\"kubernetes.azure.com/network-subscription\":\"39675fbf-5b47-472e-9bb9-5570c6edbd4f\",\"kubernetes.azure.com/node-image-version\":\"AKSAzureLinux-V2gen2-202412.10.0\",\"kubernetes.azure.com/nodenetwork-vnetguid\":\"a77c0001-505b-425f-8612-28afe0e102fc\",\"kubernetes.azure.com/nodepool-type\":\"VirtualMachineScaleSets\",\"kubernetes.azure.com/os-sku\":\"AzureLinux\",\"kubernetes.azure.com/podnetwork-type\":\"overlay\",\"kubernetes.azure.com/role\":\"agent\",\"kubernetes.io/arch\":\"amd64\",\"kubernetes.io/hostname\":\"aks-agentpool-87866486-vmss000001\",\"kubernetes.io/os\":\"linux\",\"node.kubernetes.io/instance-type\":\"Standard_D4ds_v5\",\"topology.disk.csi.azure.com/zone\":\"\",\"topology.kubernetes.io/region\":\"centralus\",\"topology.kubernetes.io/zone\":\"0\"},\"managedFields\":[{\"manager\":\"kruise-daemon\",\"operation\":\"Update\",\"apiVersion\":\"apps.kruise.io/v1alpha1\",\"time\":\"2025-01-06T22:25:55Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\".\":{},\"f:desired\":{},\"f:failed\":{},\"f:pulling\":{},\"f:succeeded\":{}}},\"subresource\":\"status\"},{\"manager\":\"kruise-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps.kruise.io/v1alpha1\",\"time\":\"2025-01-06T22:25:55Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:agentpool\":{},\"f:beta.kubernetes.io/arch\":{},\"f:beta.kubernetes.io/instance-type\":{},\"f:beta.kubernetes.io/os\":{},\"f:failure-domain.beta.kubernetes.io/region\":{},\"f:failure-domain.beta.kubernetes.io/zone\":{},\"f:kubernetes.azure.com/agentpool\":{},\"f:kubernetes.azure.com/azure-cni-overlay\":{},\"f:kubernetes.azure.com/cluster\":{},\"f:kubernetes.azure.com/consolidated-additional-properties\":{},\"f:kubernetes.azure.com/kubelet-identity-client-id\":{},\"f:kubernetes.azure.com/mode\":{},\"f:kubernetes.azure.com/network-name\":{},\"f:kubernetes.azure.com/network-policy\":{},\"f:kubernetes.azure.com/network-resourcegroup\":{},\"f:kubernetes.azure.com/network-subnet\":{},\"f:kubernetes.azure.com/network-subscription\":{},\"f:kubernetes.azure.com/node-image-version\":{},\"f:kubernetes.azure.com/nodenetwork-vnetguid\":{},\"f:kubernetes.azure.com/nodepool-type\":{},\"f:kubernetes.azure.com/os-sku\":{},\"f:kubernetes.azure.com/podnetwork-type\":{},\"f:kubernetes.azure.com/role\":{},\"f:kubernetes.io/arch\":{},\"f:kubernetes.io/hostname\":{},\"f:kubernetes.io/os\":{},\"f:node.kubernetes.io/instance-type\":{},\"f:topology.disk.csi.azure.com/zone\":{},\"f:topology.kubernetes.io/region\":{},\"f:topology.kubernetes.io/zone\":{}}},\"f:spec\":{}}}]},\"spec\":{},\"status\":{\"desired\":0,\"succeeded\":0,\"failed\":0,\"pulling\":0}}"
I0115 22:27:24.726350       1 utils.go:120] "Updating status" status="{\"desired\":0,\"succeeded\":0,\"failed\":0,\"pulling\":0}"
I0115 22:27:24.734762       1 pod_probe_controller.go:366] "NodePodProbe(%s) update status success" nodeName="aks-agentpool-87866486-vmss000001" from="{}" to="{}"
I0115 22:27:24.736771       1 imagepuller_controller.go:214] "Finished syncing" name="aks-agentpool-87866486-vmss000001"
PS C:\Git\kruise>